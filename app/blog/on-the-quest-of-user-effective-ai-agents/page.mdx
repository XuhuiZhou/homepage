import RLCurvePlot from './RLCurvePlot'
import ModelComparisonChart from './ModelComparisonChart'
import StatefulSWEChart from './StatefulSWEChart'
import CostEfficiencyChart from './CostEfficiencyChart'
import Citation from '../Citation'

export const metadata = {
  title: 'The Quest of User-Effective AI Agents',
  description: 'Exploring what makes AI agents truly effective for users, beyond benchmark performance.',
  date: '2025-11-02',
  authors: [{ name: 'Xuhui Zhou' }],
}

# The Quest of User-Effective AI Agents

<div className="not-prose mb-8 border-b border-zinc-200 pb-6">
  <div className="text-sm text-zinc-500">
    By Xuhui Zhou · Nov 2, 2025
  </div>
</div>

<Cover
  src="/images/user_effective.png"
  alt="A robot pushing a large boulder up a mountain"
  caption="The feeling of using an AI agent that is not user-effective"
/>

<Epigraph cite="Andrej Karpathy, &ldquo;We're summoning ghosts, not building animals&rdquo;">
  It's not the year of AI agents, but the decade of AI agents<Cite id="karpathy2023">Here's the [video](https://www.youtube.com/watch?v=lXUZvyajciY) of Andrej Karpathy talking about AI agents. Highly recommended.</Cite>.
</Epigraph>

{/* <div className="not-prose mb-8 rounded-lg border border-blue-200 bg-blue-50 p-4 dark:border-blue-900/30 dark:bg-blue-900/10">
  <div className="text-sm font-semibold text-blue-900 dark:text-blue-200">TL;DR</div>
  <div className="mt-2 text-sm text-blue-800 dark:text-blue-300">
    While AI agents achieve impressive benchmark scores, true effectiveness requires understanding user needs, building appropriate mental models, and designing for real-world interaction patterns. This post explores the gap between agent capabilities and user-effective AI systems.
  </div>
</div> */}

The past year has seen remarkable progress in AI agents, from coding assistants to research tools, these systems demonstrate increasingly sophisticated capabilities on standardized benchmarks. 
Yet the dream of one person commanding dozens of AI &ldquo;interns&rdquo; for 10× productivity still feels like a mirage.
In reality, users can get stuck in endless loops, pushing really hard to get the AI agent to do something, but ended up with little progress. 
Benchmark wins don’t seem to translate to real-world gains directly<Cite id="failureofbenchmarks">For some examples, see [Becker et al., 2025](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) and our work [Chen et al., 2025](https://arxiv.org/abs/2510.09801)</Cite>.
What's missing?

The dominant paradigm in training and evaluating AI agents centers on task success, overlooking the *fundamental goal of supporting real-world users*.
This post outlines our efforts to rethink that paradigm for more user-effective AI agents.

## What does &ldquo;user-effective&rdquo; mean?

There are many ways to define what makes an AI agent *user-effective*. But intuitively, we can agree on some fundamental principles, what we call the *PPP Principles*.
First, an AI agent must be able to actually do the task well when given a clear and correct instruction. That’s what most benchmarks today focus on. But beyond accuracy, user-effectiveness also means efficiency, i.e., how fast the agent can complete the task. This exactly pinpoints one of the foundations of user-effectiveness, the first P: *Productivity*.

But wait, is this enough? Not every task is a simple one-off, and not every instruction is complete or correct. Often, users themselves aren’t entirely sure what they want, let alone how to describe it precisely. 
In these cases, the agent needs to ask questions, clarify goals, and guide the user through the process of refining instructions. 
That’s where the second P: *Proactivity* comes in. 
Furthermore, a proactive agent doesn’t just seek missing information, it also helps users understand how to best work with the agent, what it produces, and how to use those results effectively. For instance, if the agent writes a Linux kernel memory management package, its job doesn’t end there, it should also help the user understand how to use it, handle edge cases, and recognize limitations. 
This kind of proactivity not only improves outcomes but also builds trust between the user and the agent.

Finally, every user has their *own* way of working with AI agents. Some prefer to give high-level language instructions; others like to co-create step by step. 
Some are comfortable letting the agent take risks; others want close oversight. 
Even stylistic preferences, like a &ldquo;pythonic&rdquo; versus &ldquo;C-style&rdquo; approach, can differ widely. That’s why there’s no one-size-fits-all agentic solution for the third P: *Personalization*. To truly be user-effective, agents must be agile and flexible to adapt to different users, contexts, and collaboration styles.

Our research shows that optimizing solely for Productivity may yield short-term gains in overall user effectiveness (across all three PPP dimensions). However, as optimization continues, it inevitably undermines the other two Ps: *Proactivity* and *Personalization* (see <Ref fig="rl-curves" />).

<Figure id="rl-curves">
  <RLCurvePlot />
</Figure>

Interestingly, the PPP principles align closely with recent research on Scaling Collaborative Effort <Cite id="scalingcollaborativeeffort">In [Shen et al., 2025](https://arxiv.org/abs/2510.25744), authors introduce *collaborative effort scaling*,
a framework that captures how an agent’s utility
grows with increasing user involvement</Cite>, where authors argue that two additional metrics are needed to truly measure how well agents collaborate with humans:
1. *User Effort* — how much cognitive and investigative work users invest in the collaboration process, which
may involve actively building an understanding of the task or the agent’s reasoning process, or simply
answering the agent’s clarification prompts;
2. *Utility of Joint Actions* — how much the joint human and agent team can accomplish together, reminiscent
of joint human-AI team performance studied in prior literature <Cite id="bansal2021">In [Bansal et al., 2021](https://arxiv.org/abs/2102.03006), authors study the performance of joint human-AI teams and find that the utility of joint actions increases with increasing user involvement</Cite>.

This perspective resonates deeply with our **PPP Principles**: Productivity captures the efficiency of joint outcomes, Proactivity shapes the interaction that drives user engagement, and Personalization determines how seamlessly the agent adapts to each user’s level of effort and collaboration style.

### Pioneering research on the proactive behavior of AI agents
In 2024, we initiated a project to investigate the question asking behavior of coding assistants <Cite id="vijayvargiya2024">For more details, see [Vijayvargiya et al., 2025](https://arxiv.org/abs/2502.13069)</Cite>.
We found that most of the software engineering (SWE) agents are not good at asking questions. In fact, they barely ask any questions at all. 

Specifically, we took the SWE-bench-verified dataset<Cite id="swebenchverified">The dataset is released by [OpenAI](https://openai.com/index/introducing-swe-bench-verified/), where they hired experienced SWE developers to verify the issue/instruction of the task is complete and clear.</Cite> and transformed the complete and clear task instruction into an incomplete and ambiguous one by applying LLM-based perturbation.
We then gave the original instruction to an LLM-simulated human user (mimicking a scenario where users have more context and knowledge about the task than the agent). The perturbed instruction was given to the agent, and we asked it to complete the task and explicitly informed the agent that it could ask the user questions if it was uncertain about any part of the task.
As shown in <Ref fig="model-comparison" />, agents powered by leading agentic models rarely asked questions (that's why they have zero false positive rate). Interestingly, models not specifically tuned for agentic tasks were the ones that actually initiated questions, even though often in incorrect ways.

<Figure id="model-comparison">
  <ModelComparisonChart />
</Figure>

Through this study, we realized a fundamental gap in how today's AI agents are trained and evaluated, which is a missing piece that limits their real-world effectiveness. 
And we are now *on a quest* to address that gap.


## The theory of mind gap

In reinforcement learning, tasks where an agent has full access to the current state of the environment are modeled as a *Markov Decision Process (MDP)*. For complex, fully-observable tasks, AI agents have already achieved superhuman performance (e.g., *AlphaGo* <Cite id="alphago">The GOAT! [Silver et al., 2016](https://www.nature.com/articles/nature16961)</Cite>). Current agentic benchmarks are *Partially Observable Markov Decision Processes (POMDPs)* <Cite id="pomdp">For more details, see [Puterman, 1994](https://www.sciencedirect.com/science/article/pii/B9780125966701500098)</Cite>, as the agent must reason under uncertainty about the environment's true state (e.g., *SWE-bench* and *BrowseComp* <Cite id="browsecomp">[BrowseComp](https://browsecomp.github.io/) is a QA benchmark requiring agents to extensively explore the webdata</Cite>). 

However, in these &ldquo;task-POMDPs&rdquo;, the underlying state is *static*, i.e., the codebase or web graph doesn't change unexpectedly. The challenge is one of *exploration and inference*. This assumption of a static, environmental hidden state breaks down in the real world, where agents must collaborate with human users. This introduces a second, fundamentally different source of partial observability. The underlying process is not just a POMDP over the *task*, but one that must also account for the *user*. The agent must now reason under uncertainty about the user's *unobservable mental state*. This human-centric POMDP has long been studied in *human-AI interaction*. Many works on *Theory of Mind (ToM)* explore precisely this issue, focusing on how agents can infer the unobservable mental states of others <Cite id="tom">For a couple of examples, see [Zhang et al., 2025](https://arxiv.org/abs/2502.15676), [Zhu et al., 2021](https://proceedings.mlr.press/v139/zhu21d.html), and our work [Zhou et al., 2024](https://arxiv.org/abs/2310.11667)</Cite>.

We hypothesize that building truly *user-effective* AI agents requires moving beyond solving just the *environmental* POMDP. Agents must also model the *human* POMDP, inferring the user’s *intent and goals*, *knowledge and belief states*, and *preferences and constraints* across long-horizon sessions.

### ToM-SWE: user mental modeling for software engineering agents
ToM-SWE<Cite id="tomswe">For more details, see [Zhou et al., 2025](https://arxiv.org/abs/2510.21903)</Cite> serves as our very first attempt to model the user's mental state and continuously learn from the user's feedback in a complex and long-horizon setting. As shown in <Ref fig="tom-swe" />, we propose this dual-agent framework where the code agent and ToM agent could be powered by different LLMs and mangage different tasks.
Specifically, the ToM agent communicates with the code agent while also agentically managing the hierarchical database to persist the user's previous conversation history and user models.
<Figure id="tom-swe">
  <figure className="fullwidth not-prose my-8">
    <img
      src="/images/tom-swe-diagram.png"
      alt="ToM-SWE system architecture showing user interaction with SWE Agent, ToM Agent, and hierarchical memory"
      className="w-full"
    />
    <figcaption className="text-center mt-3 text-sm text-zinc-600">
      Overview of the ToM-SWE framework: the SWE agent handles code generation and execution, while the ToM agent focuses on user modeling and intent inference. The SWE agent consults the ToM agent to predict the user's mental state before suggesting technical actions. Meanwhile, the ToM agent maintains an external hierarchical memory system to persist the user's state and update user models after each session (with update\_memory action).
    </figcaption>
  </figure>
</Figure>

To really evaluate agent's ability in modeling user's mental state in the long run, we build this benchmark dataset called Stateful SWE Bench, where we collected 453 real-world developer-agent sessions and created 15 distinct &ldquo;developer profiles&rdquo;, each with unique communication styles (like verbosity) and coding preferences (like testing habits). Our benchmark then uses an LLM-powered simulator to &ldquo;act&rdquo; as these different profiles.
The initial user instruction is further perturbed to create a more challenging setting, e.g., a single sentence vague description for the complex Github issue.

The agents have to correctly query the user for clarification about the tasks (*PPP Principle 2: Proactivity*).
Furthermore, the agents are given access to the user's past conversation history and must learn to adapt from those past interactions <Cite id="contextwindow">The conversation history averaged around 1.5 million tokens, thus creating a huge context window requiring efficient memory management</Cite>, e.g., ask a &ldquo;low verbosity&rdquo; user too many questions, and their satisfaction score will drop. This pushes agents to move beyond just task completion and become effective collaborators that can model and adapt to their users (*PPP Principle 3: Personalization*).

<Figure id="stateful-swe">
  <StatefulSWEChart />
</Figure>

Our ToM-SWE agent significantly outperforms baselines on the Stateful SWE Bench (<Ref fig="stateful-swe" />). More importantly, <Ref fig="cost-efficiency" /> reveals that even small LLMs, when powering the ToM agent, dramatically boost performance. This result is key: it suggests that modeling the user's mental state is a distinct and critical capability, one that can be powered by smaller, more efficient models.

To validate this in a real-world setting, we ran a three-week study where 17 developers used our ToM-enhanced CLI for their own daily coding tasks. Across 209 sessions, developers accepted (fully or partially) the ToM agent's suggestions 86% of the time, confirming its practical, real-world utility.

<Figure id="cost-efficiency">
  <CostEfficiencyChart />
</Figure>

## Reinforcing user-effective AI agents with PPP-inspired rewards

## Looking forward

This blog post was a collaboration between the authors and the very AI agents we study. The process was... interesting. We felt both the frustration of their flaws and the undeniable power of their speed, a speed no human developer can match. Despite their imperfections, one thing is clear: there is no going back.

We are at an exciting inflection point. Agents are finally generalizable enough for true human collaboration. We know that humans and AI, originating from vastly different &ldquo;training paradigms&rdquo;, have complementary strengths <Cite id="fateandfaith">For example, in [Dziri et al., 2023](https://arxiv.org/abs/2305.18654), authors study the inherent limitations of LLMs in compositionality.</Cite>. Yet, the dominant public narrative focuses on replacement, amplifying anxiety and fear <Cite id="llmreplacinghumans">There are numerous news articles reflecting this narrative, amplifying the public anxiety: [Anthropic’s new Claude can code for 30 hours. Think of it as your AI coworker](https://venturebeat.com/ai/anthropics-new-claude-can-code-for-30-hours-think-of-it-as-your-ai-coworker); [Anthropic cofounders say the likelihood of AI replacing human jobs is so high that they needed to warn the world about it](https://www.businessinsider.com/anthropic-ceo-warning-world-ai-replacing-jobs-necessary-2025-9)</Cite>. 

This fear-driven "FOMO" narrative misses the more robust and sustainable future: one built on collaboration <Cite id="hai2025">It's great to see there more recent effort from Silicon Valley betting on this more collaborative and human-centric future: [Humans& is building AI models that are better at interacting with humans](https://www.forbes.com/sites/annatong/2025/10/31/xai-researcher-early-googler-in-talks-to-raise-1-billion-at-5-billion-valuation-for-new-frontier-lab/)</Cite>. But *how* do we achieve this? How can we push the boundary of AI agents to be more user-effective and truly collaborative? Here, we hope to inspire new research in this direction by exploring the problem through three critical aspects of reinforcement learning: *context*, *priors*, and *environment*.

### More human user context
Probably the quickest fix is to bridge the *context gap*. Most collaborations break down because agents are blind to the huge chunk of context users have just "below the surface" of an instruction. This context comes from *various places*: the website we just browsed, the paper we just read, the people we just talked to, the food we just ate, etc. If we *squeeze hard enough*, we can *get* more human user context. We can capture the user's screen as visual context <Cite id="usercontext">[Shaikh et al., 2025](https://arxiv.org/abs/2505.10831)</Cite> or manage conversational memory to understand intent over time <Cite id="memgpt">[MemGPT](https://arxiv.org/abs/2310.08560)</Cite>.

We could even further push the boundary of collecting user context by capturing more fine-grained user behaviors. For example, track the trajectory of the user's mouse and keyboard movements, collecting the user's voice commands, and background environmental sounds.
In this case, the human users don't need to actively and explicitly tell the agent what to do precisely. Instead, the collected context already provides a lot of information about the user's intent and goals<Cite id="contextengineering2">[Hua, Ye, Fu, et al., 2025](https://arxiv.org/abs/2510.26493)</Cite>. The Challenge here is how to efficiently and effectively fuse these disparate, multi-modal sources into a single, unified model of &ldquo;confluence of contexts&rdquo;, how to reason about the user's mental state and intent from such context and how to provide safeguarding mechanisms to avoid the context from being leaked or misused. 

### Better user priors
Unlike formal systems like math, human behavior isn't always logical; it's often highly random and chaotic. Because of this, learning a robust *user prior* is essential for effective social navigation. Capturing observable context alone will never be enough. We cannot see the &ldquo;higher-level mental states&rdquo;, i.e., a user's true *goal, intent, or belief*. Furthermore, in long-horizon sessions, simply logging all observations would cause the context window to &ldquo;explode.&rdquo;

Humans solve this exact problem daily. We don't have full context on each other, yet we collaborate effectively. As Michael Tomasello's work explains, we do this by building *mental models* of others and establishing *shared intentionality*. The foundational skill is *joint attention*: not just seeing what the user sees, but understanding that you are *both* attending to the same object with a *shared goal* <Cite id="jointattention">[Why We Cooperate](https://mitpress.mit.edu/9780262013598/why-we-cooperate/)</Cite>.

This *mental modeling* is the key to unlocking user-effective AI and creating that &ldquo;I know what you want&rdquo; feeling. The pivotal question is *how*. Current LLM training paradigms are starved of the necessary data (like inner monologue or social reasoning), which is rarely verbalized due to reporting biases.

Can we create synthetic data to fill this gap? For example, we could learn to induce a structured model of social dynamics (like beliefs, intentions, and actions) from the &ldquo;lossy, free-form narratives&rdquo; of real-world interaction, which allows us to mid-train models on such recovered synthetic data<Cite id="socialworldmodels">[Social World Models](https://arxiv.org/abs/2509.00559)</Cite>. Building such a robust user prior is, in essence, facilitating the agent's own internal social world model. This is a critical stepping stone to placing agents in an RL environment, allowing us to train them not just on task completion, but on social effectiveness.

### More realistic user RL environments

A great deal of research has focused on building RL environments for AI agents, both for task completion <Cite id="rlenvironment">There are environments focusing on the task completion [Pan et al., 2024](https://arxiv.org/abs/2412.21139) and [Wang et al., 2025](https://arxiv.org/abs/2508.09123).</Cite> and for human-AI interaction using off-the-shelf LLMs as simulated users <Cite id="humanrl">Some examples include [Wu et al., 2025](https://arxiv.org/abs/2502.00640), and ours [Zhou et al., 2025](https://arxiv.org/abs/2510.21903)</Cite>.
This approach, however, raises a crucial question: *Are these LLM simulators actually doing a faithful job of simulating human users?*

Recent investigations suggest the answer is no. The behaviors of SOTA LLMs are often very different from those of human users <Cite id="llmuser">[Oh et al., 2025](https://arxiv.org/abs/2510.05141)</Cite>. Furthermore, they frequently suffer from &ldquo;mode collapse,&rdquo; leading to a predictable and poor diversity of behaviors that fails to challenge the agent in realistic ways.

This problem points to an obvious solution: we must build better, more realistic user simulators. This is the goal of recent work like [User-LM](https://arxiv.org/abs/2510.06552), which fine-tunes language models on large-scale, real-world human-chatbot interaction data to create more faithful and diverse personas <Cite id="wildchat">User-LM is finetuned on [WildChat](https://arxiv.org/abs/2405.01470) dataset, which collected over 1M ChatGPT interaction log from 200k users</Cite>.

But even a perfectly faithful text simulator isn't enough. A real user is not just a text-generation machine; they are an agent in their own right, driven by their own intentions and goals with diverse background context.
This suggests, we could create more realistic environments that simulate a *world of users*, each with their own distinct, long-horizon goals, memory and background beliefs.
By *situating* an agent within this complex and realistic social environment, critical collaborative behaviors like *when* to ask for help, *how* to negotiate, or *what* to clarify are expected to emerge as the agent's optimal strategy for navigating the environment and achieving its goals alongside others.


## Conclusion

The quest for user-effective AI agents is not just about better models or higher benchmark scores, it requires fundamentally rethinking how we design, evaluate, and deploy AI systems that work *with* users seamlessly. We are actively working on these aspects and if you are interested in this direction, please feel free to reach out to us.


<Citation
  author="Xuhui Zhou and Weiwei Sun"
  title="The Quest of User-Effective AI Agents"
  year="2025"
  url="https://xuhuizhou.github.io/blog/on-the-quest-of-user-effective-ai-agents"
  bibtexKey="zhou2025usereffective"
/>

---

*Thanks to collaborators for discussions that shaped this post.*
